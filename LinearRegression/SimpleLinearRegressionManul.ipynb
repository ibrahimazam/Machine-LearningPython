{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "#import libraries\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "# Slash \\ \n",
    "#read data\n",
    "path = 'ex1data1.txt'\n",
    "data = pd.read_csv(path, header=None, names=['Population', 'Profit'])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#show data details\n",
    "print('data = \\n' ,data.head(10) )\n",
    "print('**************************************')\n",
    "print('data.describe = \\n',data.describe())\n",
    "print('**************************************')\n",
    "#draw data\n",
    "data.plot(kind='scatter', x='Population', y='Profit', figsize=(4,4))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#=========================================================================\n",
    "\n",
    "# adding a new column called ones before the data\n",
    "data.insert(0, 'Ones', 1) # insert add value 0 is location ones column name 1 is the values \n",
    "print('new data = \\n' ,data.head(10) ) # will add the colmun in next row\n",
    "print('**************************************')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# separate X (training data) from y (target variable)\n",
    "cols = data.shape[1] # the Shape related on diminsen \n",
    "X = data.iloc[:,0:cols-1] # the X is input and should be in capital : all rows from col 0 and - last col\n",
    "y = data.iloc[:,cols-1:cols]\n",
    "\n",
    "print('**************************************')\n",
    "print('X data = \\n' ,X.head(10) )\n",
    "print('y data = \\n' ,y.head(10) )\n",
    "print('**************************************')\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# convert from data frames to numpy matrices\n",
    "X = np.matrix(X.values)\n",
    "y = np.matrix(y.values)\n",
    "theta = np.matrix(np.array([0,0]))\n",
    "\n",
    "print('X \\n',X)\n",
    "print('X.shape = ' , X.shape)\n",
    "print('theta \\n',theta)\n",
    "print('theta.shape = ' , theta.shape)\n",
    "print('y \\n',y)\n",
    "print('y.shape = ' , y.shape)\n",
    "print('**************************************')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$\\color{Red} Hypothesis: $     $\\color{Yellow} {h}_{0} {(x)} = \\theta _{0} + \\theta _{1}$\n",
    "\n",
    "$\\color{Red} Parameters: $    $\\color{Yellow} \\theta _{0},\\theta _{1} $\n",
    "\n",
    "$\\color{Red} Cost Functaion:$ $\\color{Yellow} J(\\theta _{0},\\theta _{1}) = \\frac{1}{2m} \\sum\\limits _{i=1} ^{m } (h _\\theta ({x} (^i)) - y (^i)) ^2 $\n",
    "\n",
    "$\\color{Red} Goal :$ $\\color{Yellow}  minimize\\ \\theta _{0},\\theta _{1} J(\\theta _{0},\\theta _{1})$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#=========================================================================\n",
    "# cost function\n",
    "def computeCost(X, y, theta):\n",
    "    z = np.power(((X * theta.T) - y), 2)\n",
    "#    print('z \\n',z)\n",
    "#    print('m ' ,len(X))\n",
    "    return np.sum(z) / (2 * len(X)) # Len return number of rows  \n",
    "\n",
    "print('computeCost(X, y, theta) = ' , computeCost(X, y, theta))\n",
    "\n",
    "print('**************************************')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Gradient Descent\n",
    "While we looking for the value of Theta 0 and Theta 1 which is going to minimize the The J Valuse which is the cost function, we will assume some value of Theta. then we will gradually minimize the values until we get the minimum value of J\n",
    "\n",
    "\n",
    "### $\\color{Yellow} \\alpha = $ Decreasing Rate OR learning rate alpha\n",
    "\n",
    "$ \\color{Yellow} \\theta _{0} := \\theta _{0} =\\alpha \\frac{1}{m} \\sum\\limits _{i=1} ^{m } (h _\\theta (x _{i}) -y _{i}) $ \n",
    "\n",
    "$ \\color{Yellow} \\theta _{1} := \\theta _{1} =\\alpha \\frac{1}{m} \\sum\\limits _{i=1} ^{m } ((h _\\theta (x _{i}) -y _{i}) x _{i}$ \n",
    "\n",
    "$\\color{Red} Hypothesis: $  $\\color{Yellow} {h} _\\theta {(x)} = \\theta _{0} + \\theta _{1} x$\n",
    "\n",
    "$\\color{Red} definition:$ $\\color{Yellow} \\theta _ 0 = y \\ intercept $ \n",
    "\n",
    "$\\color{Red} definition:$ $\\color{Yellow} \\theta _ 1 = b \\ slope $ "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# GD function\n",
    "def gradientDescent(X, y, theta, alpha, iters):\n",
    "    temp = np.matrix(np.zeros(theta.shape))\n",
    "    parameters = int(theta.ravel().shape[1])\n",
    "    cost = np.zeros(iters)\n",
    "    \n",
    "    for i in range(iters):\n",
    "        error = (X * theta.T) - y\n",
    "        \n",
    "        for j in range(parameters):\n",
    "            term = np.multiply(error, X[:,j])\n",
    "            temp[0,j] = theta[0,j] - ((alpha / len(X)) * np.sum(term))\n",
    "            \n",
    "        theta = temp\n",
    "        cost[i] = computeCost(X, y, theta)\n",
    "        \n",
    "    return theta, cost\n",
    "\n",
    "\n",
    "# initialize variables for learning rate and iterations\n",
    "alpha = 0.01\n",
    "iters = 1000\n",
    "\n",
    "# perform gradient descent to \"fit\" the model parameters\n",
    "g, cost = gradientDescent(X, y, theta, alpha, iters)\n",
    "\n",
    "print('g = ' , g)\n",
    "print('cost  = ' , cost[0:50] )\n",
    "print('computeCost = ' , computeCost(X, y, g))\n",
    "print('**************************************')\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# get best fit line\n",
    "x = np.linspace(data.Population.min(), data.Population.max(), 100)\n",
    "print('x \\n',x)\n",
    "print('g \\n',g)\n",
    "\n",
    "f = g[0, 0] + (g[0, 1] * x)\n",
    "print('f \\n',f)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# draw the line\n",
    "fig, ax = plt.subplots(figsize=(5,5))\n",
    "ax.plot(x, f, 'r', label='Prediction')\n",
    "ax.scatter(data.Population, data.Profit, label='Traning Data')\n",
    "ax.legend(loc=2)\n",
    "ax.set_xlabel('Population')\n",
    "ax.set_ylabel('Profit')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# draw error graph\n",
    "fig, ax = plt.subplots(figsize=(5,5))\n",
    "ax.plot(np.arange(iters), cost, 'r')\n",
    "ax.set_xlabel('Iterations')\n",
    "ax.set_ylabel('Cost')\n",
    "ax.set_title('Error vs. Training Epoch')\n"
   ]
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "9780809c1dcb5ebfb6c99d6eaff53e5915ef1e624894815a228c084387870e35"
  },
  "kernelspec": {
   "display_name": "Python 3.8.5 64-bit ('base': conda)",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
