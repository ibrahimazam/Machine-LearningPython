{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<span style=\"color:Orange;\">\n",
    "\n",
    "## Data Propagation in NN:\n",
    "\n",
    "<span style=\"color:Turquoise;\">\n",
    "\n",
    "\n",
    "<img src=\"images\\DataPropagation.png\" width=\"500\" height=\"300\">\n",
    "\n",
    "- The X Dose include in the layer. \n",
    "- z = Theta * X \n",
    "- a = g(-z) which is Sigmiod. \n",
    "\n",
    "## The Forward Propagation \n",
    "\n",
    "<span style=\"color:Orange;\">\n",
    "\n",
    "- Forward Propagation is the way to move from the Input layer (left) to the Output layer (right) in the neural network. The process of moving from the right to left i.e backward from the Output to the Input layer is called the Backward Propagation.\n",
    "\n",
    "***************************************\n",
    "<span style=\"color:Turquoise;\">\n",
    "\n",
    "## The Backward Propagation: \n",
    "\n",
    "<span style=\"color:Orange;\">\n",
    "\n",
    "- Backward Propagation is the preferable method of adjusting or correcting the weights to reach the minimized loss function. \n",
    "- The process called Check (Test or Evalutaion). \n",
    "\n",
    "********************************** "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<span style=\"color:Orange;\">\n",
    "\n",
    "## The Theta:\n",
    "\n",
    "<span style=\"color:Turquoise;\">\n",
    "\n",
    "- In Linear regression the theta represent the parameters of the fitted line. \n",
    "- In Regression the theta value are given by a random way and we have optimized the theta value by Gradinet     descent. \n",
    "- So in NN its little bit different will use the backward propagation. \n",
    "**************************************"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<span style=\"color:Turquoise;\">\n",
    "\n",
    "## The Forward Propagation : \n",
    "\n",
    "<span style=\"color:Orange;\">\n",
    "\n",
    "- Calculate output a, or y from inputs X. \n",
    "- by using sigmoid function. \n",
    "- provision some theta value before start. \n",
    "****************************"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "<span style=\"color:Turquoise;\">\n",
    "\n",
    "## The Backward Propagation: \n",
    "\n",
    "<span style=\"color:Orange;\">\n",
    "\n",
    "- It's happend by reverse calculation for theta values. \n",
    "- We need to find the difference betwwen actual value and predicted value. \n",
    "- Then the Derivative. \n",
    "- It used for theta optimiaztion. \n",
    "***************************************\n",
    "\n",
    "<span style=\"color:Turquoise;\">\n",
    "\n",
    "(w) ----------> a vs y \n",
    "\n",
    "(w) <---------  a vs y \n",
    "\n",
    "(w)  -------->  a vs y \n",
    "\n",
    "(w) <-------->  a vs y \n",
    "\n",
    "(w)  -------->  a vs y \n",
    "\n",
    "(w) <-------->  a vs y \n",
    "\n",
    "(w)  -------->  a vs y \n",
    "\n",
    "(w) <-------->  a vs y \n",
    "\n",
    "(w)  -------->  a vs y \n",
    "\n",
    "(w) <-------->  a vs y \n",
    "***************************************"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<span style=\"color:Turquoise;\">\n",
    "\n",
    "## Data Propagation  <---------->: \n",
    "\n",
    "<span style=\"color:Orange;\">\n",
    "\n",
    "- Start with some theta value,w,b.  \n",
    "- start with Forward Propagation, and from the results will continue to the backward propagation. \n",
    "- Then will start with theta optimization, and start with the recurrence of forward and backward propagtaion. "
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
